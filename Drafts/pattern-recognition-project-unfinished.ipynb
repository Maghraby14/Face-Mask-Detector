{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":18147,"sourceType":"datasetVersion","datasetId":13405},{"sourceId":1176415,"sourceType":"datasetVersion","datasetId":667889},{"sourceId":1187790,"sourceType":"datasetVersion","datasetId":675484}],"dockerImageVersionId":30627,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#Imports\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport cv2\nfrom scipy.spatial import distance\nimport os","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#loading haarcascade_frontalface_default.xml\nface_model = cv2.CascadeClassifier('../input/haarcascades/haarcascade_frontalface_default.xml')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n#trying it out on a sample image\nimg = cv2.imread('../input/face-mask-detection/images/maksssksksss244.png')\n\nimg = cv2.cvtColor(img, cv2.IMREAD_GRAYSCALE)\n\nfaces = face_model.detectMultiScale(img,scaleFactor=1.1, minNeighbors=4) #returns a list of (x,y,w,h) tuples\n\nout_img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR) #colored output image\n\n#plotting\nfor (x,y,w,h) in faces:\n    cv2.rectangle(out_img,(x,y),(x+w,y+h),(0,0,255),1)\nplt.figure(figsize=(12,12))\nplt.imshow(out_img)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Imports for model training\n\nfrom keras.applications.vgg19 import VGG19\nfrom keras.applications.vgg19 import preprocess_input\nfrom keras import Sequential\nfrom keras.layers import Flatten, Dense\nfrom keras.preprocessing.image import ImageDataGenerator\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Line necessary for distributing training over multiple 2 GPUs (NVIDIA T4 x2)\n\nmirrored_strategy = tf.distribute.MirroredStrategy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Define labels and our dataframe\n\ndata = pd.DataFrame(columns=['image_path', 'label'])\nlabels = {\n    '/kaggle/input/face-mask-12k-images-dataset/Face Mask Dataset/Test/WithMask' : 'WithMask',\n    '/kaggle/input/face-mask-12k-images-dataset/Face Mask Dataset/Train/WithMask' : 'WithMask',\n    '/kaggle/input/face-mask-12k-images-dataset/Face Mask Dataset/Validation/WithMask' : 'WithMask',\n    '/kaggle/input/face-mask-12k-images-dataset/Face Mask Dataset/Test/WithoutMask' : 'NoMask',\n    '/kaggle/input/face-mask-12k-images-dataset/Face Mask Dataset/Train/WithoutMask' : 'NoMask',\n    '/kaggle/input/face-mask-12k-images-dataset/Face Mask Dataset/Validation/WithoutMask' : 'NoMask'\n}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Combining the training, test and validation sets into one dataframe\n\nfor folder in labels:\n    for image_name in os.listdir(folder):\n        image_path = os.path.join(folder, image_name)\n        label = labels[folder]\n        data = pd.concat([data, pd.DataFrame({'image_path': [image_path], 'label': [label]})], ignore_index=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Save the dataframe containing the whole labeled dataset as a .csv file in the working directory\n\ndata.to_csv('dataset.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Reading the csv and converting it back to a dataframe\n\ndf = pd.read_csv(\"/kaggle/working/dataset.csv\")\nprint(df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Split the whole dataset into training and testing sets\n\ntrain_df, test_df = train_test_split(df, test_size=0.25, random_state=42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Data augmentation\ntrain_datagen = ImageDataGenerator(rescale=1.0/255, horizontal_flip=True, zoom_range=0.2, shear_range=0.2)\ntrain_generator = train_datagen.flow_from_dataframe(dataframe=train_df, x_col='image_path', y_col='label',\n                                                    target_size=(224, 224), class_mode='categorical', batch_size=32)\n\ntest_datagen = ImageDataGenerator(rescale=1.0/255)\ntest_generator = test_datagen.flow_from_dataframe(dataframe=test_df, x_col='image_path', y_col='label',\n                                                  target_size=(224, 224), class_mode='categorical', batch_size=32)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install deepface","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from deepface import DeepFace","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Defining function for model creation using VGG19 as our base model\n\ndef create_model():\n    # Load the DeepFace VGGFace model with the specified input shape\n    base_model = DeepFace.build_model('VGG-Face')\n\n    # Freeze the early layers of the VGGFace model\n    for layer in base_model.layers[:-3]:\n        layer.trainable = False\n\n    # Create a new model with the VGGFace base and additional layers\n    model = Sequential()\n    model.add(base_model)\n    model.add(Flatten())\n    model.add(Dense(2, activation='softmax'))\n\n    # Compile the model\n    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n    \n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to extract SIFT features from an image\ndef extract_sift_features(image):\n    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n    sift = cv2.SIFT_create()\n    keypoints, descriptors = sift.detectAndCompute(gray, None)\n    return descriptors","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Extract SIFT features for each image in the training set\n# train_sift_features = []\n# train_labels = []\n\nfor index, row in train_df.iterrows():\n    image_path = row['image_path']\n    label = row['label']\n    img = cv2.imread(image_path)\n    img = cv2.resize(img, (224, 224))\n#     sift_features = extract_sift_features(img)\n    \n#     # Check if features are extracted\n#     if sift_features is not None:\n#         # If multiple features are extracted, take the mean\n#         if len(sift_features) > 1:\n#             sift_features = np.mean(sift_features, axis=0, keepdims=True)\n#         train_sift_features.append(sift_features)\n#     else:\n#         # If no features are extracted, append a zero-filled array\n#         train_sift_features.append(np.zeros((1, 128), dtype=np.float32))  # Assuming 128 is the size of SIFT descriptors\n    train_labels.append(label)\n        \n    # Print the number of features extracted for the current image\n#     print(f\"Image: {image_path}, Number of SIFT features: {sift_features.shape[0] if sift_features is not None else 0}\")\n\n# Convert lists to numpy arrays\n# train_sift_features = np.vstack(train_sift_features)\n\n# Verify the shapes\n# print(\"Train SIFT Features shape:\", train_sift_features.shape)\n# print(\"Train Labels shape:\", len(train_labels))\n\n# Extract SIFT features for each image in the testing set\n# test_sift_features = []\n# test_labels = []\n\nfor index, row in test_df.iterrows():\n    image_path = row['image_path']\n    label = row['label']\n    img = cv2.imread(image_path)\n    img = cv2.resize(img, (224, 224))\n#     sift_features = extract_sift_features(img)\n    \n    # Check if features are extracted\n#     if sift_features is not None:\n#         # If multiple features are extracted, take the mean\n#         if len(sift_features) > 1:\n#             sift_features = np.mean(sift_features, axis=0, keepdims=True)\n#         test_sift_features.append(sift_features)\n#     else:\n#         # If no features are extracted, append a zero-filled array\n#         test_sift_features.append(np.zeros((1, 128), dtype=np.float32))  # Assuming 128 is the size of SIFT descriptors\n    test_labels.append(label)\n\n    # Print the number of features extracted for the current image\n#     print(f\"Image: {image_path}, Number of SIFT features: {sift_features.shape[0] if sift_features is not None else 0}\")\n\n# Convert lists to numpy arrays\n# test_sift_features = np.vstack(test_sift_features)\n\n# Verify the shapes\n# print(\"Test SIFT Features shape:\", test_sift_features.shape)\n# print(\"Test Labels shape:\", len(test_labels))","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.utils import to_categorical\n\n# Convert labels to numeric categorical labels\nlabel_mapping = {'WithMask': 0, 'NoMask': 1}  # Define your label mapping\ntrain_labels_numeric = np.array([label_mapping[label] for label in train_labels])\ntest_labels_numeric = np.array([label_mapping[label] for label in test_labels])\n\n# One-hot encode the numeric labels\ntrain_labels_onehot = to_categorical(train_labels_numeric)\ntest_labels_onehot = to_categorical(test_labels_numeric)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print(\"Train data shapes:\", train_sift_features.shape, train_labels_onehot.shape)\n# print(\"Test data shapes:\", test_sift_features.shape, test_labels_onehot.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# num_sift_features = 128  # Change this to the actual size of your SIFT descriptors\n# target_height, target_width, num_channels = 224, 224, 3\n\n# # Reshape the SIFT features\n# train_sift_features_reshaped = train_sift_features.reshape(-1, num_sift_features, 1, 1)\n# test_sift_features_reshaped = test_sift_features.reshape(-1, num_sift_features, 1, 1)\n\n# # Repeat the SIFT features along the channel axis to match the required number of channels (3)\n# train_sift_features_reshaped = np.repeat(train_sift_features_reshaped, num_channels, axis=2)\n# test_sift_features_reshaped = np.repeat(test_sift_features_reshaped, num_channels, axis=2)\n\n# # Resize the reshaped SIFT features to match the target image size\n# train_sift_features_reshaped = np.repeat(train_sift_features_reshaped, target_height, axis=3)\n# test_sift_features_reshaped = np.repeat(test_sift_features_reshaped, target_height, axis=3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with mirrored_strategy.scope():\n        model = create_model()\n        history = model.fit(train_generator, epochs=10, batch_size=32, validation_data=test_generator)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evaluate the model on the test set for this fold\ntest_loss, test_accuracy = model.evaluate(test_generator)\nprint(f\"Test Accuracy for Fold {fold}: {test_accuracy}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# @title Loss & Accuracy Visualizations\nimport matplotlib.pyplot as plt\n\n# Create a figure with two subplots\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n\n# Plot the loss\nax1.plot(history.history['loss'], label='Training Loss')\nax1.plot(history.history['val_loss'], label='Validation Loss')\nax1.legend()\nax1.set_title('Training and Validation Loss')\nax1.set_xlabel('Epochs')\nax1.set_ylabel('Loss')\n\n# Plot the accuracy\nax2.plot(history.history['accuracy'], label='Training Accuracy')\nax2.plot(history.history['val_accuracy'], label='Validation Accuracy')\nax2.legend()\nax2.set_title('Training and Validation Accuracy')\nax2.set_xlabel('Epochs')\nax2.set_ylabel('Accuracy')\n\n# Adjust layout and show the plots\nplt.tight_layout()\n\n# Save the figures in the working directory\nplt.savefig('LossVal_loss.png')\nplt.savefig('AccVal_acc.png')\n\n# Show the plots\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}